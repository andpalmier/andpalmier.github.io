<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Machine Learning and phishing, pt. 1: Decision Trees | andpalmier</title>
<meta name="keywords" content="phishing, machine learning, decision tree">
<meta name="description" content="Tuning the Decision Trees algorithm to detect phishing pages">
<meta name="author" content="">
<link rel="canonical" href="https://andpalmier.com/posts/ml-with-phishing-ep1/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3f372934d1c1362b71cc19fea0df71fd2f6c0538b2e002dcd947350384804cad.css" integrity="sha256-PzcpNNHBNitxzBn&#43;oN9x/S9sBTiy4ALc2Uc1A4SATK0=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://andpalmier.com/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://andpalmier.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://andpalmier.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://andpalmier.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://andpalmier.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Machine Learning and phishing, pt. 1: Decision Trees" />
<meta property="og:description" content="Tuning the Decision Trees algorithm to detect phishing pages" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://andpalmier.com/posts/ml-with-phishing-ep1/" /><meta property="og:image" content="https://andpalmier.com/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-05-09T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-05-09T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://andpalmier.com/papermod-cover.png"/>

<meta name="twitter:title" content="Machine Learning and phishing, pt. 1: Decision Trees"/>
<meta name="twitter:description" content="Tuning the Decision Trees algorithm to detect phishing pages"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://andpalmier.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Machine Learning and phishing, pt. 1: Decision Trees",
      "item": "https://andpalmier.com/posts/ml-with-phishing-ep1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Machine Learning and phishing, pt. 1: Decision Trees",
  "name": "Machine Learning and phishing, pt. 1: Decision Trees",
  "description": "Tuning the Decision Trees algorithm to detect phishing pages",
  "keywords": [
    "phishing", "machine learning", "decision tree"
  ],
  "articleBody": " Last week I started hunting and reporting phishing websites on Twitter (follow me here if you are interested). After some digging, I have decided that it would be interesting to use this topic to refresh my memory around the basics of Machine Learning.\nIn this series of posts I am going to use a smaller variant of this dataset to create machine learning models which (hopefully) will be able to identify a phishing website.\nPlease, note that the dataset contains the 10 ‘baseline features’ that were selected in this study. The list of features and the code of this post in form of Jupyter notebook can be found in this repository on GitHub.\nThis post has been inspired by:\nInDepth: Parameter tuning for Decision Tree Detecting phishing websites using a decision tree A simple but effective decision tree Let’s start with importing the libraries and the data. I used a csv version of the dataset, which you can find here.\nimport numpy as np from sklearn import tree # Load the training data from a CSV file training_data = np.genfromtxt('phishing_smaller.csv', delimiter=',', dtype=np.int32) The csv has 10.000 samples with 11 columns, where the last one is the label of the sample, while the other values are the features.\n# inputs are in all columns except the last one inputs = training_data[:,:-1] # outputs in the last column outputs = training_data[:, -1] We will use StratifiedKFold to keep the frequency of the classes constant during our K-fold cross-validation. The random_state parameter is used for k-fold and the classifier to reproduce the same setup for all the iterations of the model.\nfrom sklearn.model_selection import StratifiedKFold # use 10-fold skf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True) In order to evaluate how good is our classifier, I will use AUC (Area Under Curve), you can find more information about it in this video.\nHere is how to create, train and evaluate our first decision tree:\n# library for evaluating the classifier import sklearn.metrics as metrics # array to store the accuracy during k-fold cross-validation accuracy = np.array([]) # loop with splits for train_index, test_index in skf.split(inputs, outputs): # 9 folds used for training X_train, X_test = inputs[train_index], inputs[test_index] # 1 fold for testing y_train, y_test = outputs[train_index], outputs[test_index] # Create a decision tree classifier classifier = tree.DecisionTreeClassifier(random_state=0) # Train the classifier classifier.fit(X_train, y_train) # Test the classifier predictions = classifier.predict(X_test) false_positive_rate, true_positive_rate, thresholds = \\ metrics.roc_curve(y_test, predictions) # calculate classifier accuracy ROC_AUC = metrics.auc(false_positive_rate, true_positive_rate) accuracy = np.append(accuracy,ROC_AUC) print(\"ROC AUC: \"+str(np.mean(accuracy))) \u003e ROC AUC: 0.9182929859719439 Not bad, but can we improve the accuracy of this decision tree with some tuning?\nTuning: criterion and splitter If we take a look at the scikit-learn documentation for the decision tree classifiers, we can see that there are many parameters available. The first two are the criterion and splitter, having both two possible values. The supported criteria are gini (for Gini impurity) and entropy (for information gain); while the supported strategies available for splitting a node are best and random.\nIn total, we have 4 possible combinations: let’s try them to check which one performs better.\n# AUC scores for test results = [] # First= gini, best: default classifier first_classifier = tree.DecisionTreeClassifier(random_state=0 \\ ,criterion=\"gini\",splitter=\"best\") # Second= gini, random second_classifier = tree.DecisionTreeClassifier(random_state=0 \\ ,criterion=\"gini\",splitter=\"random\") # Third= entropy, best third_classifier = tree.DecisionTreeClassifier(random_state=0 \\ ,criterion=\"entropy\",splitter=\"best\") # Fourth= entropy, random fourth_classifier = tree.DecisionTreeClassifier(random_state=0 \\ ,criterion=\"entropy\",splitter=\"random\") # use same folds StratifiedKFold(n_splits=10, random_state=0, shuffle=True) for train_index, test_index in skf.split(inputs, outputs): X_train, X_test = inputs[train_index], inputs[test_index] y_train, y_test = outputs[train_index], outputs[test_index] # Train and test the first classifier first_classifier.fit(X_train, y_train) predictions = first_classifier.predict(X_test) false_positive_rate, true_positive_rate, thresholds = \\ metrics.roc_curve(y_test, predictions) # calculate classifier accuracy ROC_AUC = metrics.auc(false_positive_rate, true_positive_rate) first_accuracy = np.append(accuracy,ROC_AUC) # Train and test the second classifier second_classifier.fit(X_train, y_train) predictions = second_classifier.predict(X_test) false_positive_rate, true_positive_rate, thresholds = \\ metrics.roc_curve(y_test, predictions) # calculate classifier accuracy ROC_AUC = metrics.auc(false_positive_rate, true_positive_rate) second_accuracy= np.append(accuracy,ROC_AUC) # Train and test the third classifier third_classifier.fit(X_train, y_train) predictions = third_classifier.predict(X_test) false_positive_rate, true_positive_rate, thresholds = \\ metrics.roc_curve(y_test, predictions) # calculate classifier accuracy ROC_AUC = metrics.auc(false_positive_rate, true_positive_rate) third_accuracy= np.append(accuracy,ROC_AUC) # Train and test the fourth classifier fourth_classifier.fit(X_train, y_train) predictions = fourth_classifier.predict(X_test) false_positive_rate, true_positive_rate, thresholds = \\ metrics.roc_curve(y_test, predictions) # calculate classifier accuracy ROC_AUC = metrics.auc(false_positive_rate, true_positive_rate) fourth_accuracy= np.append(accuracy,ROC_AUC) print(\"Test AUC for 'gini, best': \",np.mean(first_accuracy)) print(\"Test AUC for 'gini, random': \",np.mean(second_accuracy)) print(\"Test AUC for 'entropy, best': \",np.mean(third_accuracy)) print(\"Test AUC for 'entropy, random': \",np.mean(fourth_accuracy)) \u003e Test AUC for 'gini, best': 0.9186236108580798 \u003e Test AUC for 'gini, random': 0.9185325195846237 \u003e Test AUC for 'entropy, best': 0.9184414283111678 \u003e Test AUC for 'entropy, random': 0.9190781563126251 In this case, the fourth combination of criterion and splitter (criterion=entropy and split=random) seems to increase the performance of the classifier.\nTuning: max depth Another parameter of the decision tree that we can tune is max_depth, which indicates the maximum depth of the tree. By default, this is is set to None, which means that nodes are expanded until all leaves are pure or contain less than min_sample_split samples.\nConsidering that we have 10 parameters, we will test the performances of trees having max_depths between 1 and 10.\n# AUC scores for training and test training_results = [] test_results = [] # use same folds StratifiedKFold(n_splits=10, random_state=0, shuffle=True) # from 1 to 10 max_depths = range(1,11) for i in max_depths: # loop with splits for train_index, test_index in skf.split(inputs, outputs): training_accuracy = np.array([]) test_accuracy = np.array([]) X_train, X_test = inputs[train_index], inputs[test_index] y_train, y_test = outputs[train_index], outputs[test_index] # Create a decision tree classifier classifier = tree.DecisionTreeClassifier(random_state=0,max_depth=i) # Train the classifier classifier.fit(X_train, y_train) # Accuracy of the classifier during training training_predictions = classifier.predict(X_train) false_positive_rate, true_positive_rate, thresholds = \\ metrics.roc_curve(y_train, training_predictions) ROC_AUC = metrics.auc(false_positive_rate, true_positive_rate) training_accuracy = np.append(training_accuracy,ROC_AUC) # Test the classifier testing_predictions = classifier.predict(X_test) false_positive_rate, true_positive_rate, thresholds = \\ metrics.roc_curve(y_test, testing_predictions) # Accuracy of the classifier during test ROC_AUC = metrics.auc(false_positive_rate, true_positive_rate) test_accuracy = np.append(test_accuracy,ROC_AUC) # append results for line chart training_results.append(np.mean(training_accuracy)) test_results.append(np.mean(test_accuracy)) In order to visualize the results, let’s use matplotlib to draw a line chart.\n# training results in blue line1, = plt.plot(max_depths, training_results, 'b', label='Train AUC') # test results in red line2, = plt.plot(max_depths, test_results, 'r', label='Test AUC') plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)}) plt.ylabel('AUC score') plt.xlabel('Tree depth') plt.show() Performance of the model while tuning max_depth\nAs expected, increasing max_depth allows the model to be more specific when predicting the class of the given sample, thus improving the accuracy during training and test.\nTuning: min samples split The next parameter is min_samples_split:\nIf int, it represents the minimum number of samples required to split an internal node. If float, it is considered a fraction and ceil(min_samples_split * len(samples)) are the minimum number of samples for each split. While the default value is 2, we will test the performance of our classifier having min_samples_split between 0.05 and 1.0.\n# AUC scores for training and test training_results = [] test_results = [] # use same folds StratifiedKFold(n_splits=10, random_state=0, shuffle=True) # from 5% to 100% min_samples_splits = np.linspace(0.05, 1.0,20,endpoint=True) for i in min_samples_splits: # loop with splits for train_index, test_index in skf.split(inputs, outputs): training_accuracy = np.array([]) test_accuracy = np.array([]) X_train, X_test = inputs[train_index], inputs[test_index] y_train, y_test = outputs[train_index], outputs[test_index] # Create a decision tree classifier classifier = tree.DecisionTreeClassifier(random_state=0,min_samples_split=i) # Train the classifier classifier.fit(X_train, y_train) # Accuracy of the classifier during training training_predictions = classifier.predict(X_train) false_positive_rate, true_positive_rate, thresholds = \\ metrics.roc_curve(y_train, training_predictions) ROC_AUC = metrics.auc(false_positive_rate, true_positive_rate) training_accuracy = np.append(training_accuracy,ROC_AUC) # Test the classifier testing_predictions = classifier.predict(X_test) false_positive_rate, true_positive_rate, thresholds = \\ metrics.roc_curve(y_test, testing_predictions) # Accuracy of the classifier during test ROC_AUC = metrics.auc(false_positive_rate, true_positive_rate) test_accuracy = np.append(test_accuracy,ROC_AUC) # append results for line chart training_results.append(np.mean(training_accuracy)) test_results.append(np.mean(test_accuracy)) Let’s use another line chart to visualize the results:\n# plot line chart line1, = plt.plot(min_samples_splits, training_results, 'b', label='Train AUC') line2, = plt.plot(min_samples_splits, test_results, 'r', label='Test AUC') plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)}) plt.ylabel('AUC score') plt.xlabel('min samples split') plt.show() Performance of the model while tuning min_samples_split\nWe can clearly see from the chart how increasing min_samples_split results in an underfitting case, where the model is not able to learn from the samples during training.\nTuning: min samples leaf Similarly to the previous parameter, min_samples_leaf can be:\nint, and it is used to specify the minimum number of samples required to be at a leaf node if float, it represents a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node By default, the value is set to 1, but we will consider the cases where it goes from 0.05 to 0.5.\n# AUC scores for training and test training_results = [] test_results = [] # from 5% to 50% min_samples_leaves = np.linspace(0.05, 0.5, 10,endpoint=True) for i in min_samples_leaves: StratifiedKFold(n_splits=10, random_state=0, shuffle=True) # loop with splits for train_index, test_index in skf.split(inputs, outputs): training_accuracy = np.array([]) test_accuracy = np.array([]) X_train, X_test = inputs[train_index], inputs[test_index] y_train, y_test = outputs[train_index], outputs[test_index] # Create a decision tree classifier classifier = tree.DecisionTreeClassifier(random_state=0, min_samples_leaf=i) # Train the classifier classifier.fit(X_train, y_train) # Accuracy of the classifier during training training_predictions = classifier.predict(X_train) false_positive_rate, true_positive_rate, thresholds = \\ metrics.roc_curve(y_train, training_predictions) ROC_AUC = metrics.auc(false_positive_rate, true_positive_rate) training_accuracy = np.append(training_accuracy,ROC_AUC) # Test the classifier testing_predictions = classifier.predict(X_test) false_positive_rate, true_positive_rate, thresholds = \\ metrics.roc_curve(y_test, testing_predictions) # Accuracy of the classifier during test ROC_AUC = metrics.auc(false_positive_rate, true_positive_rate) test_accuracy = np.append(test_accuracy,ROC_AUC) # append results for line chart training_results.append(np.mean(training_accuracy)) test_results.append(np.mean(test_accuracy)) # plot line chart line1, = plt.plot(min_samples_leaves, training_results, 'b', label='Train AUC') line2, = plt.plot(min_samples_leaves, test_results, 'r', label='Test AUC') plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)}) plt.ylabel('AUC score') plt.xlabel('min samples leaf') plt.show() Performance of the model while tuning min_samples_leaf\nWe can see that, similarly to the tuning of min_samples_split, increasing min_samples_leaf cause our model to underfit, drastically affecting the accuracy of the classifier during training and test.\nTuning: max features The last parameter we are going to consider is max_features, which specifies the number of features to consider when looking for the best split.\nIf int, then consider max_features features at each split. If float, is a fraction and int(max_features * n_features) features are considered at each split. By default it is None, and max_features=n_features Considering the number of features of our dataset, we will test measure the precision of classifiers having max_features between 1 and 10.\n# AUC scores for training and test training_results = [] test_results = [] # from 1 to 10 features max_features = list(range(1,len(inputs[0])+1)) for i in max_features: StratifiedKFold(n_splits=10, random_state=0, shuffle=True) # loop with splits for train_index, test_index in skf.split(inputs, outputs): training_accuracy = np.array([]) test_accuracy = np.array([]) X_train, X_test = inputs[train_index], inputs[test_index] y_train, y_test = outputs[train_index], outputs[test_index] # Create a decision tree classifier classifier = tree.DecisionTreeClassifier(random_state=0,max_features=i) # Train the classifier classifier.fit(X_train, y_train) # Accuracy of the classifier during training training_predictions = classifier.predict(X_train) false_positive_rate, true_positive_rate, thresholds = \\ metrics.roc_curve(y_train, training_predictions) ROC_AUC = metrics.auc(false_positive_rate, true_positive_rate) training_accuracy = np.append(training_accuracy,ROC_AUC) # Accuracy of the classifier during test testing_predictions = classifier.predict(X_test) false_positive_rate, true_positive_rate, thresholds = \\ metrics.roc_curve(y_test, testing_predictions) # calculate classifier accuracy for test ROC_AUC = metrics.auc(false_positive_rate, true_positive_rate) test_accuracy = np.append(test_accuracy,ROC_AUC) # append results for line chart training_results.append(np.mean(training_accuracy)) test_results.append(np.mean(test_accuracy)) # plot line chart line1, = plt.plot(max_features, training_results, 'b', label='Train AUC') line2, = plt.plot(max_features, test_results, 'r', label='Test AUC') plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)}) plt.ylabel('AUC score') plt.xlabel('max features') plt.show() Performance of the model while tuning max_features\nWe can see how the accuracy of the model does not seem to improve much when increasing the number of features considered during a split. While this may seem counter-intuitive, the scikit-learn documentation specifies that ‘the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.’\nConclusion These posts will investigate how tuning some of the available parameters can affect the performance of simple models. In this case, we saw how criterion, splitter, max_depth, min_samples_split, min_samples_leaf and max_features alter the predictions of a decision tree.\nAs pointed out from a friend, this is not the proper way of tuning the parameters of a model: one could extend parameters search by means of the RandomizedSearchCV provided by sklearn.\n",
  "wordCount" : "1966",
  "inLanguage": "en",
  "datePublished": "2020-05-09T00:00:00Z",
  "dateModified": "2020-05-09T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://andpalmier.com/posts/ml-with-phishing-ep1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "andpalmier",
    "logo": {
      "@type": "ImageObject",
      "url": "https://andpalmier.com/images/favicon-32x32.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://andpalmier.com" accesskey="h" title="andpalmier (Alt + H)">andpalmier</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://andpalmier.com/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://andpalmier.com/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Machine Learning and phishing, pt. 1: Decision Trees
    </h1>
    <div class="post-meta"><span title='2020-05-09 00:00:00 +0000 UTC'>May 9, 2020</span>&nbsp;·&nbsp;10 min

</div>
  </header> 
  <div class="post-content"><figure class="align-center ">
    <img loading="lazy" src="/images/posts/ml-with-phishing/jaws.jpg#center"
         alt="phishing like in jaws"/> 
</figure>

<p>Last week I started hunting and reporting phishing websites on Twitter (follow me <a href="https://twitter.com/andpalmier" target="_blank" rel="noopener">here</a> if you are interested). After some digging, I have decided that it would be interesting to use this topic to refresh my memory around the basics of Machine Learning.</p>
<p>In this series of posts I am going to use a smaller variant of <a href="https://data.mendeley.com/datasets/h3cgnj8hft/1/" target="_blank" rel="noopener">this dataset</a> to create machine learning models which (hopefully) will be able to identify a phishing website.</p>
<p>Please, note that the dataset contains the 10 &lsquo;<em>baseline features</em>&rsquo; that were selected in <a href="https://www.sciencedirect.com/science/article/pii/S0020025519300763" target="_blank" rel="noopener">this study</a>.
The list of features and the code of this post in form of Jupyter notebook can be found in this <a href="https://github.com/andpalmier/MLWithPhishing" target="_blank" rel="noopener">repository on GitHub</a>.</p>
<p>This post has been inspired by:</p>
<ul>
<li><a href="https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3" target="_blank" rel="noopener">InDepth: Parameter tuning for Decision Tree</a></li>
<li><a href="https://medium.com/@NicolasPapernot/detecting-phishing-websites-using-a-decision-tree-ed069d073723" target="_blank" rel="noopener">Detecting phishing websites using a decision tree</a></li>
</ul>
<h2 id="a-simple-but-effective-decision-tree">A simple but effective decision tree<a hidden class="anchor" aria-hidden="true" href="#a-simple-but-effective-decision-tree">#</a></h2>
<p>Let&rsquo;s start with importing the libraries and the data. I used a csv version of the dataset, which you can find <a href="https://github.com/andpalmier/MLWithPhishing" target="_blank" rel="noopener">here</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Load the training data from a CSV file</span>
</span></span><span class="line"><span class="cl"><span class="n">training_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s1">&#39;phishing_smaller.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span></span></code></pre></div><p>The csv has 10.000 samples with 11 columns, where the last one is the label of the sample, while the other values are the features.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># inputs are in all columns except the last one</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">training_data</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># outputs in the last column</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">training_data</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span></code></pre></div><p>We will use <code>StratifiedKFold</code> to keep the frequency of the classes constant during our K-fold cross-validation. The <code>random_state</code> parameter is used for k-fold and the classifier to reproduce the same setup for all the iterations of the model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># use 10-fold</span>
</span></span><span class="line"><span class="cl"><span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></div><p>In order to evaluate how good is our classifier, I will use AUC (Area Under Curve), you can find more information about it <a href="https://www.youtube.com/watch?v=OAl6eAyP-yo" target="_blank" rel="noopener">in this video</a>.</p>
<p>Here is how to create, train and evaluate our first decision tree:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># library for evaluating the classifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">metrics</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># array to store the accuracy during k-fold cross-validation</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># loop with splits</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 9 folds used for training</span>
</span></span><span class="line"><span class="cl">    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 1 fold for testing</span>
</span></span><span class="line"><span class="cl">    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">outputs</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Create a decision tree classifier</span>
</span></span><span class="line"><span class="cl">    <span class="n">classifier</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Train the classifier</span>
</span></span><span class="line"><span class="cl">    <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Test the classifier</span>
</span></span><span class="line"><span class="cl">    <span class="n">predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">        <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># calculate classifier accuracy</span>
</span></span><span class="line"><span class="cl">    <span class="n">ROC_AUC</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span><span class="n">ROC_AUC</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;ROC AUC: &#34;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)))</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&gt; ROC AUC: 0.9182929859719439
</span></span></code></pre></div><p>Not bad, but can we improve the accuracy of this decision tree with some tuning?</p>
<h2 id="tuning-criterion-and-splitter">Tuning: criterion and splitter<a hidden class="anchor" aria-hidden="true" href="#tuning-criterion-and-splitter">#</a></h2>
<p>If we take a look at <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" target="_blank" rel="noopener">the scikit-learn documentation for the decision tree classifiers</a>, we can see that there are many parameters available. The first two are the <code>criterion</code> and <code>splitter</code>, having both two possible values. The supported criteria are <code>gini</code> (for <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" target="_blank" rel="noopener">Gini impurity</a>) and <code>entropy</code> (for <a href="https://en.wikipedia.org/wiki/Information_gain_in_decision_trees" target="_blank" rel="noopener">information gain</a>); while the supported strategies available for splitting a node are <code>best</code> and <code>random</code>.</p>
<p>In total, we have 4 possible combinations: let&rsquo;s try them to check which one performs better.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python3" data-lang="python3"><span class="line"><span class="cl"><span class="c1"># AUC scores for test</span>
</span></span><span class="line"><span class="cl"><span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># First= gini, best: default classifier</span>
</span></span><span class="line"><span class="cl"><span class="n">first_classifier</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span> \
</span></span><span class="line"><span class="cl">    <span class="p">,</span><span class="n">criterion</span><span class="o">=</span><span class="s2">&#34;gini&#34;</span><span class="p">,</span><span class="n">splitter</span><span class="o">=</span><span class="s2">&#34;best&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Second= gini, random</span>
</span></span><span class="line"><span class="cl"><span class="n">second_classifier</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span> \
</span></span><span class="line"><span class="cl">    <span class="p">,</span><span class="n">criterion</span><span class="o">=</span><span class="s2">&#34;gini&#34;</span><span class="p">,</span><span class="n">splitter</span><span class="o">=</span><span class="s2">&#34;random&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Third= entropy, best</span>
</span></span><span class="line"><span class="cl"><span class="n">third_classifier</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span> \
</span></span><span class="line"><span class="cl">    <span class="p">,</span><span class="n">criterion</span><span class="o">=</span><span class="s2">&#34;entropy&#34;</span><span class="p">,</span><span class="n">splitter</span><span class="o">=</span><span class="s2">&#34;best&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Fourth= entropy, random</span>
</span></span><span class="line"><span class="cl"><span class="n">fourth_classifier</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span> \
</span></span><span class="line"><span class="cl">    <span class="p">,</span><span class="n">criterion</span><span class="o">=</span><span class="s2">&#34;entropy&#34;</span><span class="p">,</span><span class="n">splitter</span><span class="o">=</span><span class="s2">&#34;random&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># use same folds</span>
</span></span><span class="line"><span class="cl"><span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">outputs</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Train and test the first classifier</span>
</span></span><span class="line"><span class="cl">    <span class="n">first_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">predictions</span> <span class="o">=</span> <span class="n">first_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">		<span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># calculate classifier accuracy</span>
</span></span><span class="line"><span class="cl">    <span class="n">ROC_AUC</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">first_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span><span class="n">ROC_AUC</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Train and test the second classifier</span>
</span></span><span class="line"><span class="cl">    <span class="n">second_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">predictions</span> <span class="o">=</span> <span class="n">second_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">		<span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># calculate classifier accuracy</span>
</span></span><span class="line"><span class="cl">    <span class="n">ROC_AUC</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">second_accuracy</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span><span class="n">ROC_AUC</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Train and test the third classifier</span>
</span></span><span class="line"><span class="cl">    <span class="n">third_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">predictions</span> <span class="o">=</span> <span class="n">third_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">		<span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># calculate classifier accuracy</span>
</span></span><span class="line"><span class="cl">    <span class="n">ROC_AUC</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">third_accuracy</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span><span class="n">ROC_AUC</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Train and test the fourth classifier</span>
</span></span><span class="line"><span class="cl">    <span class="n">fourth_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">predictions</span> <span class="o">=</span> <span class="n">fourth_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">		<span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># calculate classifier accuracy</span>
</span></span><span class="line"><span class="cl">    <span class="n">ROC_AUC</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">fourth_accuracy</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span><span class="n">ROC_AUC</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Test AUC for &#39;gini, best&#39;:       &#34;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">first_accuracy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Test AUC for &#39;gini, random&#39;:     &#34;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">second_accuracy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Test AUC for &#39;entropy, best&#39;:    &#34;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">third_accuracy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Test AUC for &#39;entropy, random&#39;:  &#34;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fourth_accuracy</span><span class="p">))</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&gt; Test AUC for &#39;gini, best&#39;:        0.9186236108580798
</span></span><span class="line"><span class="cl">&gt; Test AUC for &#39;gini, random&#39;:      0.9185325195846237
</span></span><span class="line"><span class="cl">&gt; Test AUC for &#39;entropy, best&#39;:     0.9184414283111678
</span></span><span class="line"><span class="cl">&gt; Test AUC for &#39;entropy, random&#39;:   0.9190781563126251
</span></span></code></pre></div><p>In this case, the fourth combination of criterion and splitter (<code>criterion=entropy</code> and <code>split=random</code>) seems to increase the performance of the classifier.</p>
<h2 id="tuning-max-depth">Tuning: max depth<a hidden class="anchor" aria-hidden="true" href="#tuning-max-depth">#</a></h2>
<p>Another parameter of the decision tree that we can tune is <code>max_depth</code>, which indicates the maximum depth of the tree. By default, this is is set to <code>None</code>, which means that nodes are expanded until all leaves are pure or contain less than <code>min_sample_split</code> samples.</p>
<p>Considering that we have 10 parameters, we will test the performances of trees having <code>max_depths</code> between 1 and 10.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># AUC scores for training and test</span>
</span></span><span class="line"><span class="cl"><span class="n">training_results</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">test_results</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># use same folds</span>
</span></span><span class="line"><span class="cl"><span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># from 1 to 10</span>
</span></span><span class="line"><span class="cl"><span class="n">max_depths</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">max_depths</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># loop with splits</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">training_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
</span></span><span class="line"><span class="cl">        <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">outputs</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Create a decision tree classifier</span>
</span></span><span class="line"><span class="cl">        <span class="n">classifier</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">max_depth</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Train the classifier</span>
</span></span><span class="line"><span class="cl">        <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Accuracy of the classifier during training</span>
</span></span><span class="line"><span class="cl">        <span class="n">training_predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">            <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">training_predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">ROC_AUC</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">training_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_accuracy</span><span class="p">,</span><span class="n">ROC_AUC</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Test the classifier</span>
</span></span><span class="line"><span class="cl">        <span class="n">testing_predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">            <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">testing_predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Accuracy of the classifier during test</span>
</span></span><span class="line"><span class="cl">        <span class="n">ROC_AUC</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">,</span><span class="n">ROC_AUC</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># append results for line chart</span>
</span></span><span class="line"><span class="cl">    <span class="n">training_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">training_accuracy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">test_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">))</span>
</span></span></code></pre></div><p>In order to visualize the results, let&rsquo;s use <code>matplotlib</code> to draw a line chart.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># training results in blue</span>
</span></span><span class="line"><span class="cl"><span class="n">line1</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">max_depths</span><span class="p">,</span> <span class="n">training_results</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train AUC&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># test results in red</span>
</span></span><span class="line"><span class="cl"><span class="n">line2</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">max_depths</span><span class="p">,</span> <span class="n">test_results</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test AUC&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handler_map</span><span class="o">=</span><span class="p">{</span><span class="n">line1</span><span class="p">:</span> <span class="n">HandlerLine2D</span><span class="p">(</span><span class="n">numpoints</span><span class="o">=</span><span class="mi">2</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;AUC score&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Tree depth&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><figure class="align-center ">
    <img loading="lazy" src="/images/posts/ml-with-phishing/ep1/maxdepth.png#center"
         alt="max depth chart"/> <figcaption>
            <p>Performance of the model while tuning max_depth</p>
        </figcaption>
</figure>

<p>As expected, increasing <code>max_depth</code> allows the model to be more specific when predicting the class of the given sample, thus improving the accuracy during training and test.</p>
<h2 id="tuning-min-samples-split">Tuning: min samples split<a hidden class="anchor" aria-hidden="true" href="#tuning-min-samples-split">#</a></h2>
<p>The next parameter is <code>min_samples_split</code>:</p>
<ul>
<li>If <code>int</code>, it represents the minimum number of samples required to split an internal node.</li>
<li>If <code>float</code>, it is considered a fraction and <code>ceil(min_samples_split * len(samples))</code> are the minimum number of samples for each split.</li>
</ul>
<p>While the default value is 2, we will test the performance of our classifier having <code>min_samples_split</code> between 0.05 and 1.0.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># AUC scores for training and test</span>
</span></span><span class="line"><span class="cl"><span class="n">training_results</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">test_results</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># use same folds</span>
</span></span><span class="line"><span class="cl"><span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># from 5% to 100%</span>
</span></span><span class="line"><span class="cl"><span class="n">min_samples_splits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">endpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">min_samples_splits</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># loop with splits</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">training_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
</span></span><span class="line"><span class="cl">        <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
</span></span><span class="line"><span class="cl">        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">outputs</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Create a decision tree classifier</span>
</span></span><span class="line"><span class="cl">        <span class="n">classifier</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">min_samples_split</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Train the classifier</span>
</span></span><span class="line"><span class="cl">        <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Accuracy of the classifier during training</span>
</span></span><span class="line"><span class="cl">        <span class="n">training_predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">            <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">training_predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">ROC_AUC</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">training_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_accuracy</span><span class="p">,</span><span class="n">ROC_AUC</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Test the classifier</span>
</span></span><span class="line"><span class="cl">        <span class="n">testing_predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">            <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">testing_predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Accuracy of the classifier during test</span>
</span></span><span class="line"><span class="cl">        <span class="n">ROC_AUC</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">,</span><span class="n">ROC_AUC</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># append results for line chart</span>
</span></span><span class="line"><span class="cl">    <span class="n">training_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">training_accuracy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">test_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">))</span>
</span></span></code></pre></div><p>Let&rsquo;s use another line chart to visualize the results:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># plot line chart</span>
</span></span><span class="line"><span class="cl"><span class="n">line1</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">min_samples_splits</span><span class="p">,</span> <span class="n">training_results</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train AUC&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">line2</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">min_samples_splits</span><span class="p">,</span> <span class="n">test_results</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test AUC&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handler_map</span><span class="o">=</span><span class="p">{</span><span class="n">line1</span><span class="p">:</span> <span class="n">HandlerLine2D</span><span class="p">(</span><span class="n">numpoints</span><span class="o">=</span><span class="mi">2</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;AUC score&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;min samples split&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><figure class="align-center ">
    <img loading="lazy" src="/images/posts/ml-with-phishing/ep1/minsamplessplit.png#center"
         alt="min samples split chart"/> <figcaption>
            <p>Performance of the model while tuning min_samples_split</p>
        </figcaption>
</figure>

<p>We can clearly see from the chart how increasing <code>min_samples_split</code> results in an underfitting case, where the model is not able to learn from the samples during training.</p>
<h2 id="tuning-min-samples-leaf">Tuning: min samples leaf<a hidden class="anchor" aria-hidden="true" href="#tuning-min-samples-leaf">#</a></h2>
<p>Similarly to the previous parameter, <code>min_samples_leaf</code> can be:</p>
<ul>
<li><code>int</code>, and it is used to specify the minimum number of samples required to be at a leaf node</li>
<li>if <code>float</code>, it represents a fraction and <code>ceil(min_samples_leaf * n_samples)</code> are the minimum number of samples for each node</li>
</ul>
<p>By default, the value is set to 1, but we will consider the cases where it goes from 0.05 to 0.5.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># AUC scores for training and test</span>
</span></span><span class="line"><span class="cl"><span class="n">training_results</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">test_results</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># from 5% to 50%</span>
</span></span><span class="line"><span class="cl"><span class="n">min_samples_leaves</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span><span class="n">endpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">min_samples_leaves</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># loop with splits</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">training_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
</span></span><span class="line"><span class="cl">        <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
</span></span><span class="line"><span class="cl">        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">outputs</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Create a decision tree classifier</span>
</span></span><span class="line"><span class="cl">        <span class="n">classifier</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Train the classifier</span>
</span></span><span class="line"><span class="cl">        <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Accuracy of the classifier during training</span>
</span></span><span class="line"><span class="cl">        <span class="n">training_predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">            <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">training_predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">ROC_AUC</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">training_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_accuracy</span><span class="p">,</span><span class="n">ROC_AUC</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Test the classifier</span>
</span></span><span class="line"><span class="cl">        <span class="n">testing_predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">            <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">testing_predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Accuracy of the classifier during test</span>
</span></span><span class="line"><span class="cl">        <span class="n">ROC_AUC</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">,</span><span class="n">ROC_AUC</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># append results for line chart</span>
</span></span><span class="line"><span class="cl">    <span class="n">training_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">training_accuracy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">test_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot line chart</span>
</span></span><span class="line"><span class="cl"><span class="n">line1</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">min_samples_leaves</span><span class="p">,</span> <span class="n">training_results</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train AUC&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">line2</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">min_samples_leaves</span><span class="p">,</span> <span class="n">test_results</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test AUC&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handler_map</span><span class="o">=</span><span class="p">{</span><span class="n">line1</span><span class="p">:</span> <span class="n">HandlerLine2D</span><span class="p">(</span><span class="n">numpoints</span><span class="o">=</span><span class="mi">2</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;AUC score&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;min samples leaf&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><figure class="align-center ">
    <img loading="lazy" src="/images/posts/ml-with-phishing/ep1/minsamplesleaf.png#center"
         alt="min samples leaf chart"/> <figcaption>
            <p>Performance of the model while tuning min_samples_leaf</p>
        </figcaption>
</figure>

<p>We can see that, similarly to the tuning of <code>min_samples_split</code>, increasing <code>min_samples_leaf</code> cause our model to underfit, drastically affecting the accuracy of the classifier during training and test.</p>
<h2 id="tuning-max-features">Tuning: max features<a hidden class="anchor" aria-hidden="true" href="#tuning-max-features">#</a></h2>
<p>The last parameter we are going to consider is <code>max_features</code>, which specifies the number of features to consider when looking for the best split.</p>
<ul>
<li>If <code>int</code>, then consider <code>max_features</code> features at each split.</li>
<li>If <code>float</code>, is a fraction and <code>int(max_features * n_features)</code> features are considered at each split.</li>
<li>By default it is <code>None</code>, and <code>max_features=n_features</code></li>
</ul>
<p>Considering the number of features of our dataset, we will test measure the precision of classifiers having <code>max_features</code> between 1 and 10.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># AUC scores for training and test</span>
</span></span><span class="line"><span class="cl"><span class="n">training_results</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">test_results</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># from 1 to 10 features</span>
</span></span><span class="line"><span class="cl"><span class="n">max_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">max_features</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># loop with splits</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">training_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
</span></span><span class="line"><span class="cl">        <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
</span></span><span class="line"><span class="cl">        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">outputs</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Create a decision tree classifier</span>
</span></span><span class="line"><span class="cl">        <span class="n">classifier</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">max_features</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Train the classifier</span>
</span></span><span class="line"><span class="cl">        <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Accuracy of the classifier during training</span>
</span></span><span class="line"><span class="cl">        <span class="n">training_predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">            <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">training_predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">ROC_AUC</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">training_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_accuracy</span><span class="p">,</span><span class="n">ROC_AUC</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Accuracy of the classifier during test</span>
</span></span><span class="line"><span class="cl">        <span class="n">testing_predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">            <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">testing_predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># calculate classifier accuracy for test</span>
</span></span><span class="line"><span class="cl">        <span class="n">ROC_AUC</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">,</span><span class="n">ROC_AUC</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># append results for line chart</span>
</span></span><span class="line"><span class="cl">    <span class="n">training_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">training_accuracy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">test_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot line chart</span>
</span></span><span class="line"><span class="cl"><span class="n">line1</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">max_features</span><span class="p">,</span> <span class="n">training_results</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train AUC&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">line2</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">max_features</span><span class="p">,</span> <span class="n">test_results</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test AUC&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handler_map</span><span class="o">=</span><span class="p">{</span><span class="n">line1</span><span class="p">:</span> <span class="n">HandlerLine2D</span><span class="p">(</span><span class="n">numpoints</span><span class="o">=</span><span class="mi">2</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;AUC score&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;max features&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><figure class="align-center ">
    <img loading="lazy" src="/images/posts/ml-with-phishing/ep1/maxfeatures.png#center"
         alt="max features chart"/> <figcaption>
            <p>Performance of the model while tuning max_features</p>
        </figcaption>
</figure>

<p>We can see how the accuracy of the model does not seem to improve much when increasing the number of features considered during a split. While this may seem counter-intuitive, the scikit-learn documentation specifies that &lsquo;<em>the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features</em>.&rsquo;</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>These posts will investigate how tuning some of the available parameters can affect the performance of simple models. In this case, we saw how <code>criterion</code>, <code>splitter</code>, <code>max_depth</code>, <code>min_samples_split</code>, <code>min_samples_leaf</code> and <code>max_features</code> alter the predictions of a decision tree.</p>
<p>As pointed out from a friend, this is not the proper way of tuning the parameters of a model: one could extend parameters search by means of the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" target="_blank" rel="noopener">RandomizedSearchCV</a> provided by sklearn.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://andpalmier.com/tags/phishing/">phishing</a></li>
      <li><a href="https://andpalmier.com/tags/machine-learning/">machine learning</a></li>
      <li><a href="https://andpalmier.com/tags/decision-tree/">decision tree</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://andpalmier.com">andpalmier</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js" integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<script>
const images = Array.from(document.querySelectorAll(".post-content img"));
images.forEach(img => {
  mediumZoom(img, {
    margin: 0,  
    scrollOffset: 40,  
    container: null,  
    template: null,  
    background: 'rgba(0, 0, 0, 0.8)'
  });
});
</script>


<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
