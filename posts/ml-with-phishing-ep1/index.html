<!DOCTYPE html>
<html lang="en">

    <head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Language" content="en">

	<meta name="author" content="Andrea Palmieri">
	<meta name="description" content="Last week I started hunting and reporting phishing websites on Twitter (follow me here if you are interested). After some digging, I have decided that it would be interesting to use this topic to refresh my memory around the basics of Machine Learning.
In this series of posts I am going to use a smaller variant of this dataset to create machine learning models which (hopefully) will be able to identify a phishing website.">
	<meta name="keywords" content="blog, infosec, security, cyber security">

	<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Machine Learning and phishing, pt.1: decision tree"/>
<meta name="twitter:description" content="Last week I started hunting and reporting phishing websites on Twitter (follow me here if you are interested). After some digging, I have decided that it would be interesting to use this topic to refresh my memory around the basics of Machine Learning.
In this series of posts I am going to use a smaller variant of this dataset to create machine learning models which (hopefully) will be able to identify a phishing website."/>

	<meta property="og:title" content="Machine Learning and phishing, pt.1: decision tree" />
<meta property="og:description" content="Last week I started hunting and reporting phishing websites on Twitter (follow me here if you are interested). After some digging, I have decided that it would be interesting to use this topic to refresh my memory around the basics of Machine Learning.
In this series of posts I am going to use a smaller variant of this dataset to create machine learning models which (hopefully) will be able to identify a phishing website." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/ml-with-phishing-ep1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-05-09T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-05-09T00:00:00+00:00" />



	
	<base href="/posts/ml-with-phishing-ep1/">
	
	<title>
  Machine Learning and phishing, pt.1: decision tree · andpalmier
</title>

	
	<link rel="canonical" href="/posts/ml-with-phishing-ep1/">
	

	<link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css" integrity="sha384-Bfad6CLCknfcloXFOyFnlgtENryhrpZCe29RTifKEixXQZ38WheV+i/6YWSzkz3V" crossorigin="anonymous">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

	
	
	
	<link rel="stylesheet" href="/css/coder.min.d5c572a4dda2996f45f4bd9e584d43751439ce2134c33701e8d32920cc8e7d92.css" integrity="sha256-1cVypN2imW9F9L2eWE1DdRQ5ziE0wzcB6NMpIMyOfZI=" crossorigin="anonymous" media="screen" />
	

	

	
	
	
	
	<link rel="stylesheet" href="/css/coder-dark.min.58ba09b2c45d36fcfc1a96c8ad83bbbc03a9de4bce8463f3c30a896134febfb3.css" integrity="sha256-WLoJssRdNvz8GpbIrYO7vAOp3kvOhGPzwwqJYTT&#43;v7M=" crossorigin="anonymous" media="screen" />
	
	

	
	<link rel="stylesheet" href="/css/extra-style.css" />
	

	

	<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
	<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

	

	<meta name="generator" content="Hugo 0.89.4" />
    </head>

    
    
    
    
    <body class="colorscheme-dark"

	  onload=""
	  >
	  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


	  <main class="wrapper">
	      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      andpalmier
    </a>
    
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="/">Home</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="/about/">About</a>
          </li>
        
      
      
    </ul>
    
  </section>
</nav>


	      <div class="content">
		  
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Machine Learning and phishing, pt.1: decision tree</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fas fa-calendar"></i>
              <time datetime='2020-05-09T00:00:00Z'>
                May 9, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fas fa-clock"></i>
              10-minute read
            </span>
          </div>
          <div class="categories">
  <i class="fas fa-folder"></i>
    <a href="/categories/phishing/">phishing</a></div>

          <div class="tags">
  <i class="fas fa-tag"></i>
    <a href="/tags/phishing/">phishing</a>
      <span class="separator">•</span>
    <a href="/tags/machine-learning/">machine learning</a>
      <span class="separator">•</span>
    <a href="/tags/decision-tree/">decision tree</a></div>

        </div>
      </header>

      <div>
        
        <figure><img src="/images/posts/ml-with-phishing/jaws.jpg"/>
</figure>

<p>Last week I started hunting and reporting phishing websites on Twitter (follow me <a href="https://twitter.com/andpalmier">here</a> if you are interested). After some digging, I have decided that it would be interesting to use this topic to refresh my memory around the basics of Machine Learning.</p>
<p>In this series of posts I am going to use a smaller variant of <a href="https://data.mendeley.com/datasets/h3cgnj8hft/1/">this dataset</a> to create machine learning models which (hopefully) will be able to identify a phishing website.</p>
<p>Please, note that the dataset contains the 10 &lsquo;<em>baseline features</em>&rsquo; that were selected in <a href="https://www.sciencedirect.com/science/article/pii/S0020025519300763">this study</a>.
The list of features and the code of this post in form of Jupyter notebook can be found in this <a href="https://github.com/andpalmier/MLWithPhishing">repository on GitHub</a>.</p>
<p>This post has been inspired by:</p>
<ul>
<li><a href="https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3">InDepth: Parameter tuning for Decision Tree</a></li>
<li><a href="https://medium.com/@NicolasPapernot/detecting-phishing-websites-using-a-decision-tree-ed069d073723">Detecting phishing websites using a decision tree</a></li>
</ul>
<h2 id="a-simple-but-effective-decision-tree">A simple but effective decision tree</h2>
<p>Let&rsquo;s start with importing the libraries and the data. I used a csv version of the dataset, which you can find <a href="https://github.com/andpalmier/MLWithPhishing">here</a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
<span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> tree

<span style="color:#75715e"># Load the training data from a CSV file</span>
training_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>genfromtxt(<span style="color:#e6db74">&#39;phishing_smaller.csv&#39;</span>, delimiter<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;,&#39;</span>, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>int32)
</code></pre></div><p>The csv has 10.000 samples with 11 columns, where the last one is the label of the sample, while the other values are the features.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># inputs are in all columns except the last one</span>
inputs <span style="color:#f92672">=</span> training_data[:,:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]

<span style="color:#75715e"># outputs in the last column</span>
outputs <span style="color:#f92672">=</span> training_data[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
</code></pre></div><p>We will use <code>StratifiedKFold</code> to keep the frequency of the classes constant during our K-fold cross-validation. The <code>random_state</code> parameter is used for k-fold and the classifier to reproduce the same setup for all the iterations of the model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> StratifiedKFold

<span style="color:#75715e"># use 10-fold</span>
skf <span style="color:#f92672">=</span> StratifiedKFold(n_splits<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</code></pre></div><p>In order to evaluate how good is our classifier, I will use AUC (Area Under Curve), you can find more information about it <a href="https://www.youtube.com/watch?v=OAl6eAyP-yo">in this video</a>.</p>
<p>Here is how to create, train and evaluate our first decision tree:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># library for evaluating the classifier</span>
<span style="color:#f92672">import</span> sklearn.metrics <span style="color:#66d9ef">as</span> metrics

<span style="color:#75715e"># array to store the accuracy during k-fold cross-validation</span>
accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([])

<span style="color:#75715e"># loop with splits</span>
<span style="color:#66d9ef">for</span> train_index, test_index <span style="color:#f92672">in</span> skf<span style="color:#f92672">.</span>split(inputs, outputs):

    <span style="color:#75715e"># 9 folds used for training</span>
    X_train, X_test <span style="color:#f92672">=</span> inputs[train_index], inputs[test_index]
    <span style="color:#75715e"># 1 fold for testing</span>
    y_train, y_test <span style="color:#f92672">=</span> outputs[train_index], outputs[test_index]

    <span style="color:#75715e"># Create a decision tree classifier</span>
    classifier <span style="color:#f92672">=</span> tree<span style="color:#f92672">.</span>DecisionTreeClassifier(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)

    <span style="color:#75715e"># Train the classifier</span>
    classifier<span style="color:#f92672">.</span>fit(X_train, y_train)

    <span style="color:#75715e"># Test the classifier</span>
    predictions <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>predict(X_test)
    false_positive_rate, true_positive_rate, thresholds <span style="color:#f92672">=</span> \
        metrics<span style="color:#f92672">.</span>roc_curve(y_test, predictions)

    <span style="color:#75715e"># calculate classifier accuracy</span>
    ROC_AUC <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>auc(false_positive_rate, true_positive_rate)
    accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(accuracy,ROC_AUC)

print(<span style="color:#e6db74">&#34;ROC AUC: &#34;</span><span style="color:#f92672">+</span>str(np<span style="color:#f92672">.</span>mean(accuracy)))
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">&gt; ROC AUC: 0.9182929859719439
</code></pre></div><p>Not bad, but can we improve the accuracy of this decision tree with some tuning?</p>
<h2 id="tuning-criterion-and-splitter">Tuning: criterion and splitter</h2>
<p>If we take a look at <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">the scikit-learn documentation for the decision tree classifiers</a>, we can see that there are many parameters available. The first two are the <code>criterion</code> and <code>splitter</code>, having both two possible values. The supported criteria are <code>gini</code> (for <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity">Gini impurity</a>) and <code>entropy</code> (for <a href="https://en.wikipedia.org/wiki/Information_gain_in_decision_trees">information gain</a>); while the supported strategies available for splitting a node are <code>best</code> and <code>random</code>.</p>
<p>In total, we have 4 possible combinations: let&rsquo;s try them to check which one performs better.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#75715e"># AUC scores for test</span>
results <span style="color:#f92672">=</span> []

<span style="color:#75715e"># First= gini, best: default classifier</span>
first_classifier <span style="color:#f92672">=</span> tree<span style="color:#f92672">.</span>DecisionTreeClassifier(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> \
    ,criterion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gini&#34;</span>,splitter<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;best&#34;</span>)

<span style="color:#75715e"># Second= gini, random</span>
second_classifier <span style="color:#f92672">=</span> tree<span style="color:#f92672">.</span>DecisionTreeClassifier(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> \
    ,criterion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gini&#34;</span>,splitter<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;random&#34;</span>)

<span style="color:#75715e"># Third= entropy, best</span>
third_classifier <span style="color:#f92672">=</span> tree<span style="color:#f92672">.</span>DecisionTreeClassifier(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> \
    ,criterion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;entropy&#34;</span>,splitter<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;best&#34;</span>)

<span style="color:#75715e"># Fourth= entropy, random</span>
fourth_classifier <span style="color:#f92672">=</span> tree<span style="color:#f92672">.</span>DecisionTreeClassifier(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> \
    ,criterion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;entropy&#34;</span>,splitter<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;random&#34;</span>)

<span style="color:#75715e"># use same folds</span>
StratifiedKFold(n_splits<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
<span style="color:#66d9ef">for</span> train_index, test_index <span style="color:#f92672">in</span> skf<span style="color:#f92672">.</span>split(inputs, outputs):

    X_train, X_test <span style="color:#f92672">=</span> inputs[train_index], inputs[test_index]
    y_train, y_test <span style="color:#f92672">=</span> outputs[train_index], outputs[test_index]

    <span style="color:#75715e"># Train and test the first classifier</span>
    first_classifier<span style="color:#f92672">.</span>fit(X_train, y_train)
    predictions <span style="color:#f92672">=</span> first_classifier<span style="color:#f92672">.</span>predict(X_test)
    false_positive_rate, true_positive_rate, thresholds <span style="color:#f92672">=</span> \
		metrics<span style="color:#f92672">.</span>roc_curve(y_test, predictions)
    <span style="color:#75715e"># calculate classifier accuracy</span>
    ROC_AUC <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>auc(false_positive_rate, true_positive_rate)
    first_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(accuracy,ROC_AUC)

    <span style="color:#75715e"># Train and test the second classifier</span>
    second_classifier<span style="color:#f92672">.</span>fit(X_train, y_train)
    predictions <span style="color:#f92672">=</span> second_classifier<span style="color:#f92672">.</span>predict(X_test)
    false_positive_rate, true_positive_rate, thresholds <span style="color:#f92672">=</span> \
		metrics<span style="color:#f92672">.</span>roc_curve(y_test, predictions)
    <span style="color:#75715e"># calculate classifier accuracy</span>
    ROC_AUC <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>auc(false_positive_rate, true_positive_rate)
    second_accuracy<span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(accuracy,ROC_AUC)

    <span style="color:#75715e"># Train and test the third classifier</span>
    third_classifier<span style="color:#f92672">.</span>fit(X_train, y_train)
    predictions <span style="color:#f92672">=</span> third_classifier<span style="color:#f92672">.</span>predict(X_test)
    false_positive_rate, true_positive_rate, thresholds <span style="color:#f92672">=</span> \
		metrics<span style="color:#f92672">.</span>roc_curve(y_test, predictions)
    <span style="color:#75715e"># calculate classifier accuracy</span>
    ROC_AUC <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>auc(false_positive_rate, true_positive_rate)
    third_accuracy<span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(accuracy,ROC_AUC)

    <span style="color:#75715e"># Train and test the fourth classifier</span>
    fourth_classifier<span style="color:#f92672">.</span>fit(X_train, y_train)
    predictions <span style="color:#f92672">=</span> fourth_classifier<span style="color:#f92672">.</span>predict(X_test)
    false_positive_rate, true_positive_rate, thresholds <span style="color:#f92672">=</span> \
		metrics<span style="color:#f92672">.</span>roc_curve(y_test, predictions)
    <span style="color:#75715e"># calculate classifier accuracy</span>
    ROC_AUC <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>auc(false_positive_rate, true_positive_rate)
    fourth_accuracy<span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(accuracy,ROC_AUC)

print(<span style="color:#e6db74">&#34;Test AUC for &#39;gini, best&#39;:       &#34;</span>,np<span style="color:#f92672">.</span>mean(first_accuracy))
print(<span style="color:#e6db74">&#34;Test AUC for &#39;gini, random&#39;:     &#34;</span>,np<span style="color:#f92672">.</span>mean(second_accuracy))
print(<span style="color:#e6db74">&#34;Test AUC for &#39;entropy, best&#39;:    &#34;</span>,np<span style="color:#f92672">.</span>mean(third_accuracy))
print(<span style="color:#e6db74">&#34;Test AUC for &#39;entropy, random&#39;:  &#34;</span>,np<span style="color:#f92672">.</span>mean(fourth_accuracy))
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">&gt; Test AUC for &#39;gini, best&#39;:        0.9186236108580798
&gt; Test AUC for &#39;gini, random&#39;:      0.9185325195846237
&gt; Test AUC for &#39;entropy, best&#39;:     0.9184414283111678
&gt; Test AUC for &#39;entropy, random&#39;:   0.9190781563126251
</code></pre></div><p>In this case, the fourth combination of criterion and splitter (<code>criterion=entropy</code> and <code>split=random</code>) seems to increase the performance of the classifier.</p>
<h2 id="tuning-max-depth">Tuning: max depth</h2>
<p>Another parameter of the decision tree that we can tune is <code>max_depth</code>, which indicates the maximum depth of the tree. By default, this is is set to <code>None</code>, which means that nodes are expanded until all leaves are pure or contain less than <code>min_sample_split</code> samples.</p>
<p>Considering that we have 10 parameters, we will test the performances of trees having <code>max_depths</code> between 1 and 10.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># AUC scores for training and test</span>
training_results <span style="color:#f92672">=</span> []
test_results <span style="color:#f92672">=</span> []

<span style="color:#75715e"># use same folds</span>
StratifiedKFold(n_splits<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

<span style="color:#75715e"># from 1 to 10</span>
max_depths <span style="color:#f92672">=</span> range(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">11</span>)

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> max_depths:

    <span style="color:#75715e"># loop with splits</span>
    <span style="color:#66d9ef">for</span> train_index, test_index <span style="color:#f92672">in</span> skf<span style="color:#f92672">.</span>split(inputs, outputs):
        training_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([])
        test_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([])

        X_train, X_test <span style="color:#f92672">=</span> inputs[train_index], inputs[test_index]
        y_train, y_test <span style="color:#f92672">=</span> outputs[train_index], outputs[test_index]

        <span style="color:#75715e"># Create a decision tree classifier</span>
        classifier <span style="color:#f92672">=</span> tree<span style="color:#f92672">.</span>DecisionTreeClassifier(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,max_depth<span style="color:#f92672">=</span>i)

        <span style="color:#75715e"># Train the classifier</span>
        classifier<span style="color:#f92672">.</span>fit(X_train, y_train)

        <span style="color:#75715e"># Accuracy of the classifier during training</span>
        training_predictions <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>predict(X_train)
        false_positive_rate, true_positive_rate, thresholds <span style="color:#f92672">=</span> \
            metrics<span style="color:#f92672">.</span>roc_curve(y_train, training_predictions)
        ROC_AUC <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>auc(false_positive_rate, true_positive_rate)
        training_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(training_accuracy,ROC_AUC)

        <span style="color:#75715e"># Test the classifier</span>
        testing_predictions <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>predict(X_test)
        false_positive_rate, true_positive_rate, thresholds <span style="color:#f92672">=</span> \
            metrics<span style="color:#f92672">.</span>roc_curve(y_test, testing_predictions)

        <span style="color:#75715e"># Accuracy of the classifier during test</span>
        ROC_AUC <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>auc(false_positive_rate, true_positive_rate)
        test_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(test_accuracy,ROC_AUC)

    <span style="color:#75715e"># append results for line chart</span>
    training_results<span style="color:#f92672">.</span>append(np<span style="color:#f92672">.</span>mean(training_accuracy))
    test_results<span style="color:#f92672">.</span>append(np<span style="color:#f92672">.</span>mean(test_accuracy))
</code></pre></div><p>In order to visualize the results, let&rsquo;s use <code>matplotlib</code> to draw a line chart.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># training results in blue</span>
line1, <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>plot(max_depths, training_results, <span style="color:#e6db74">&#39;b&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Train AUC&#39;</span>)
<span style="color:#75715e"># test results in red</span>
line2, <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>plot(max_depths, test_results, <span style="color:#e6db74">&#39;r&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Test AUC&#39;</span>)
plt<span style="color:#f92672">.</span>legend(handler_map<span style="color:#f92672">=</span>{line1: HandlerLine2D(numpoints<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)})
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;AUC score&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Tree depth&#39;</span>)
plt<span style="color:#f92672">.</span>show()
</code></pre></div><figure><img src="/images/posts/ml-with-phishing/ep1/maxdepth.png"
         alt="Performance of the model while tuning max_depth"/><figcaption>
            <p>Performance of the model while tuning max_depth</p>
        </figcaption>
</figure>

<p>As expected, increasing <code>max_depth</code> allows the model to be more specific when predicting the class of the given sample, thus improving the accuracy during training and test.</p>
<h2 id="tuning-min-samples-split">Tuning: min samples split</h2>
<p>The next parameter is <code>min_samples_split</code>:</p>
<ul>
<li>If <code>int</code>, it represents the minimum number of samples required to split an internal node.</li>
<li>If <code>float</code>, it is considered a fraction and <code>ceil(min_samples_split * len(samples))</code> are the minimum number of samples for each split.</li>
</ul>
<p>While the default value is 2, we will test the performance of our classifier having <code>min_samples_split</code> between 0.05 and 1.0.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># AUC scores for training and test</span>
training_results <span style="color:#f92672">=</span> []
test_results <span style="color:#f92672">=</span> []

<span style="color:#75715e"># use same folds</span>
StratifiedKFold(n_splits<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

<span style="color:#75715e"># from 5% to 100%</span>
min_samples_splits <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0.05</span>, <span style="color:#ae81ff">1.0</span>,<span style="color:#ae81ff">20</span>,endpoint<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> min_samples_splits:

    <span style="color:#75715e"># loop with splits</span>
    <span style="color:#66d9ef">for</span> train_index, test_index <span style="color:#f92672">in</span> skf<span style="color:#f92672">.</span>split(inputs, outputs):
        training_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([])
        test_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([])
        X_train, X_test <span style="color:#f92672">=</span> inputs[train_index], inputs[test_index]
        y_train, y_test <span style="color:#f92672">=</span> outputs[train_index], outputs[test_index]

        <span style="color:#75715e"># Create a decision tree classifier</span>
        classifier <span style="color:#f92672">=</span> tree<span style="color:#f92672">.</span>DecisionTreeClassifier(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,min_samples_split<span style="color:#f92672">=</span>i)

        <span style="color:#75715e"># Train the classifier</span>
        classifier<span style="color:#f92672">.</span>fit(X_train, y_train)

        <span style="color:#75715e"># Accuracy of the classifier during training</span>
        training_predictions <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>predict(X_train)
        false_positive_rate, true_positive_rate, thresholds <span style="color:#f92672">=</span> \
            metrics<span style="color:#f92672">.</span>roc_curve(y_train, training_predictions)
        ROC_AUC <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>auc(false_positive_rate, true_positive_rate)
        training_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(training_accuracy,ROC_AUC)

        <span style="color:#75715e"># Test the classifier</span>
        testing_predictions <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>predict(X_test)
        false_positive_rate, true_positive_rate, thresholds <span style="color:#f92672">=</span> \
            metrics<span style="color:#f92672">.</span>roc_curve(y_test, testing_predictions)

        <span style="color:#75715e"># Accuracy of the classifier during test</span>
        ROC_AUC <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>auc(false_positive_rate, true_positive_rate)
        test_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(test_accuracy,ROC_AUC)

    <span style="color:#75715e"># append results for line chart</span>
    training_results<span style="color:#f92672">.</span>append(np<span style="color:#f92672">.</span>mean(training_accuracy))
    test_results<span style="color:#f92672">.</span>append(np<span style="color:#f92672">.</span>mean(test_accuracy))
</code></pre></div><p>Let&rsquo;s use another line chart to visualize the results:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># plot line chart</span>
line1, <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>plot(min_samples_splits, training_results, <span style="color:#e6db74">&#39;b&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Train AUC&#39;</span>)
line2, <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>plot(min_samples_splits, test_results, <span style="color:#e6db74">&#39;r&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Test AUC&#39;</span>)
plt<span style="color:#f92672">.</span>legend(handler_map<span style="color:#f92672">=</span>{line1: HandlerLine2D(numpoints<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)})
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;AUC score&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;min samples split&#39;</span>)
plt<span style="color:#f92672">.</span>show()
</code></pre></div><figure><img src="/images/posts/ml-with-phishing/ep1/minsamplessplit.png"
         alt="Performance of the model while tuning min_samples_split"/><figcaption>
            <p>Performance of the model while tuning min_samples_split</p>
        </figcaption>
</figure>

<p>We can clearly see from the chart how increasing <code>min_samples_split</code> results in an underfitting case, where the model is not able to learn from the samples during training.</p>
<h2 id="tuning-min-samples-leaf">Tuning: min samples leaf</h2>
<p>Similarly to the previous parameter, <code>min_samples_leaf</code> can be:</p>
<ul>
<li><code>int</code>, and it is used to specify the minimum number of samples required to be at a leaf node</li>
<li>if <code>float</code>, it represents a fraction and <code>ceil(min_samples_leaf * n_samples)</code> are the minimum number of samples for each node</li>
</ul>
<p>By default, the value is set to 1, but we will consider the cases where it goes from 0.05 to 0.5.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># AUC scores for training and test</span>
training_results <span style="color:#f92672">=</span> []
test_results <span style="color:#f92672">=</span> []

<span style="color:#75715e"># from 5% to 50%</span>
min_samples_leaves <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0.05</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">10</span>,endpoint<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> min_samples_leaves:

    StratifiedKFold(n_splits<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

    <span style="color:#75715e"># loop with splits</span>
    <span style="color:#66d9ef">for</span> train_index, test_index <span style="color:#f92672">in</span> skf<span style="color:#f92672">.</span>split(inputs, outputs):
        training_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([])
        test_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([])
        X_train, X_test <span style="color:#f92672">=</span> inputs[train_index], inputs[test_index]
        y_train, y_test <span style="color:#f92672">=</span> outputs[train_index], outputs[test_index]

        <span style="color:#75715e"># Create a decision tree classifier</span>
        classifier <span style="color:#f92672">=</span> tree<span style="color:#f92672">.</span>DecisionTreeClassifier(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, min_samples_leaf<span style="color:#f92672">=</span>i)

        <span style="color:#75715e"># Train the classifier</span>
        classifier<span style="color:#f92672">.</span>fit(X_train, y_train)

        <span style="color:#75715e"># Accuracy of the classifier during training</span>
        training_predictions <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>predict(X_train)
        false_positive_rate, true_positive_rate, thresholds <span style="color:#f92672">=</span> \
            metrics<span style="color:#f92672">.</span>roc_curve(y_train, training_predictions)
        ROC_AUC <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>auc(false_positive_rate, true_positive_rate)
        training_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(training_accuracy,ROC_AUC)

        <span style="color:#75715e"># Test the classifier</span>
        testing_predictions <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>predict(X_test)
        false_positive_rate, true_positive_rate, thresholds <span style="color:#f92672">=</span> \
            metrics<span style="color:#f92672">.</span>roc_curve(y_test, testing_predictions)

        <span style="color:#75715e"># Accuracy of the classifier during test</span>
        ROC_AUC <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>auc(false_positive_rate, true_positive_rate)
        test_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(test_accuracy,ROC_AUC)

    <span style="color:#75715e"># append results for line chart</span>
    training_results<span style="color:#f92672">.</span>append(np<span style="color:#f92672">.</span>mean(training_accuracy))
    test_results<span style="color:#f92672">.</span>append(np<span style="color:#f92672">.</span>mean(test_accuracy))

<span style="color:#75715e"># plot line chart</span>
line1, <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>plot(min_samples_leaves, training_results, <span style="color:#e6db74">&#39;b&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Train AUC&#39;</span>)
line2, <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>plot(min_samples_leaves, test_results, <span style="color:#e6db74">&#39;r&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Test AUC&#39;</span>)
plt<span style="color:#f92672">.</span>legend(handler_map<span style="color:#f92672">=</span>{line1: HandlerLine2D(numpoints<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)})
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;AUC score&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;min samples leaf&#39;</span>)
plt<span style="color:#f92672">.</span>show()
</code></pre></div><figure><img src="/images/posts/ml-with-phishing/ep1/minsamplesleaf.png"
         alt="Performance of the model while tuning min_samples_leaf"/><figcaption>
            <p>Performance of the model while tuning min_samples_leaf</p>
        </figcaption>
</figure>

<p>We can see that, similarly to the tuning of <code>min_samples_split</code>, increasing <code>min_samples_leaf</code> cause our model to underfit, drastically affecting the accuracy of the classifier during training and test.</p>
<h2 id="tuning-max-features">Tuning: max features</h2>
<p>The last parameter we are going to consider is <code>max_features</code>, which specifies the number of features to consider when looking for the best split.</p>
<ul>
<li>If <code>int</code>, then consider <code>max_features</code> features at each split.</li>
<li>If <code>float</code>, is a fraction and <code>int(max_features * n_features)</code> features are considered at each split.</li>
<li>By default it is <code>None</code>, and <code>max_features=n_features</code></li>
</ul>
<p>Considering the number of features of our dataset, we will test measure the precision of classifiers having <code>max_features</code> between 1 and 10.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># AUC scores for training and test</span>
training_results <span style="color:#f92672">=</span> []
test_results <span style="color:#f92672">=</span> []

<span style="color:#75715e"># from 1 to 10 features</span>
max_features <span style="color:#f92672">=</span> list(range(<span style="color:#ae81ff">1</span>,len(inputs[<span style="color:#ae81ff">0</span>])<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>))

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> max_features:

    StratifiedKFold(n_splits<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

    <span style="color:#75715e"># loop with splits</span>
    <span style="color:#66d9ef">for</span> train_index, test_index <span style="color:#f92672">in</span> skf<span style="color:#f92672">.</span>split(inputs, outputs):
        training_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([])
        test_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([])
        X_train, X_test <span style="color:#f92672">=</span> inputs[train_index], inputs[test_index]
        y_train, y_test <span style="color:#f92672">=</span> outputs[train_index], outputs[test_index]

        <span style="color:#75715e"># Create a decision tree classifier</span>
        classifier <span style="color:#f92672">=</span> tree<span style="color:#f92672">.</span>DecisionTreeClassifier(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,max_features<span style="color:#f92672">=</span>i)

        <span style="color:#75715e"># Train the classifier</span>
        classifier<span style="color:#f92672">.</span>fit(X_train, y_train)

        <span style="color:#75715e"># Accuracy of the classifier during training</span>
        training_predictions <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>predict(X_train)
        false_positive_rate, true_positive_rate, thresholds <span style="color:#f92672">=</span> \
            metrics<span style="color:#f92672">.</span>roc_curve(y_train, training_predictions)
        ROC_AUC <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>auc(false_positive_rate, true_positive_rate)
        training_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(training_accuracy,ROC_AUC)

        <span style="color:#75715e"># Accuracy of the classifier during test</span>
        testing_predictions <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>predict(X_test)
        false_positive_rate, true_positive_rate, thresholds <span style="color:#f92672">=</span> \
            metrics<span style="color:#f92672">.</span>roc_curve(y_test, testing_predictions)

        <span style="color:#75715e"># calculate classifier accuracy for test</span>
        ROC_AUC <span style="color:#f92672">=</span> metrics<span style="color:#f92672">.</span>auc(false_positive_rate, true_positive_rate)
        test_accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(test_accuracy,ROC_AUC)

    <span style="color:#75715e"># append results for line chart</span>
    training_results<span style="color:#f92672">.</span>append(np<span style="color:#f92672">.</span>mean(training_accuracy))
    test_results<span style="color:#f92672">.</span>append(np<span style="color:#f92672">.</span>mean(test_accuracy))

<span style="color:#75715e"># plot line chart</span>
line1, <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>plot(max_features, training_results, <span style="color:#e6db74">&#39;b&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Train AUC&#39;</span>)
line2, <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>plot(max_features, test_results, <span style="color:#e6db74">&#39;r&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Test AUC&#39;</span>)
plt<span style="color:#f92672">.</span>legend(handler_map<span style="color:#f92672">=</span>{line1: HandlerLine2D(numpoints<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)})
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;AUC score&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;max features&#39;</span>)
plt<span style="color:#f92672">.</span>show()
</code></pre></div><figure><img src="/images/posts/ml-with-phishing/ep1/maxfeatures.png"
         alt="Performance of the model while tuning max_features"/><figcaption>
            <p>Performance of the model while tuning max_features</p>
        </figcaption>
</figure>

<p>We can see how the accuracy of the model does not seem to improve much when increasing the number of features considered during a split. While this may seem counter-intuitive, the scikit-learn documentation specifies that &lsquo;<em>the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features</em>.&rsquo;</p>
<h2 id="conclusion">Conclusion</h2>
<p>These posts will investigate how tuning some of the available parameters can affect the performance of simple models. In this case, we saw how <code>criterion</code>, <code>splitter</code>, <code>max_depth</code>, <code>min_samples_split</code>, <code>min_samples_leaf</code> and <code>max_features</code> alter the predictions of a decision tree.</p>
<p>As pointed out from a friend, this is not the proper way of tuning the parameters of a model: one could extend parameters search by means of the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html">RandomizedSearchCV</a> provided by sklearn.</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js" id="MathJax-script"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [
          ['$', '$'], ['\\(', '\\)']
        ],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  </section>

	      </div>

	<footer class="footer">

<hr>

    <section class="container">
	
	<p>I am far from being an expert and I like to learn new things. <br> If you have comments or suggestions, feel free to ping me!</p>
	
	
	
	


<ul>
    
    
    <li>
	<a href="https://twitter.com/andpalmier/" aria-label="Twitter"  target="_blank" >
	    <i class="fab fa-twitter" aria-hidden="true"></i>
	</a>
    </li>
    
    
    
    <li>
	<a href="https://github.com/andpalmier/" aria-label="Github"  target="_blank" >
	    <i class="fab fa-github" aria-hidden="true"></i>
	</a>
    </li>
    
    
    
    <li>
	<a href="https://www.linkedin.com/in/andreapalmieri95/" aria-label="LinkedIn"  target="_blank" >
	    <i class="fab fa-linkedin" aria-hidden="true"></i>
	</a>
    </li>
    
    
    
    <li>
	<a href="mailto:andpalmier@gmail.com" aria-label="Mail"  target="_blank" >
	    <i class="fas fa-envelope" aria-hidden="true"></i>
	</a>
    </li>
    
    
    
</ul>

    </section>
</footer>


	  </main>

    
      
        
        <script src="/js/dark-mode.min.aee9c8a464eb7b3534c7110f7c5e169e7039e2fd92710e0626d451d6725af137.js"></script>
      
    

	  

	  

	  

    </body>


</html>
